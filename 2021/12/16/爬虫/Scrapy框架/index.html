<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Scrapy介绍 | John Doe</title><meta name="keywords" content="爬虫"><meta name="author" content="John Doe"><meta name="copyright" content="John Doe"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Scrapy介绍12345678910111213141516171819#1  通用的网络爬虫框架,爬虫界的django#2 scrapy执行流程    5大组件        -引擎(EGINE)：大总管，负责控制数据的流向        -调度器(SCHEDULER)：由它来决定下一个要抓取的网址是什么，去除重复的网址(集合，布隆过滤器)            -深度优先">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy介绍">
<meta property="og:url" content="http://www.zhangxiaotao.live/2021/12/16/%E7%88%AC%E8%99%AB/Scrapy%E6%A1%86%E6%9E%B6/index.html">
<meta property="og:site_name" content="John Doe">
<meta property="og:description" content="Scrapy介绍12345678910111213141516171819#1  通用的网络爬虫框架,爬虫界的django#2 scrapy执行流程    5大组件        -引擎(EGINE)：大总管，负责控制数据的流向        -调度器(SCHEDULER)：由它来决定下一个要抓取的网址是什么，去除重复的网址(集合，布隆过滤器)            -深度优先">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto">
<meta property="article:published_time" content="2021-12-16T12:14:00.000Z">
<meta property="article:modified_time" content="2021-12-16T13:33:16.857Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto"><link rel="shortcut icon" href="/img/avatar.png"><link rel="canonical" href="http://www.zhangxiaotao.live/2021/12/16/%E7%88%AC%E8%99%AB/Scrapy%E6%A1%86%E6%9E%B6/"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="//busuanzi.ibruce.info"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.10.5/dist/instantsearch.min.js" defer=""></script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"B0YYEXE704","apiKey":"df3b5032cce15f8c2fe870edf0ff7d98","indexName":"blog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容：${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":500},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Scrapy介绍',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-12-16 21:33:16'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/mouse.css"><style>#article-container.post-content h1:before, h2:before, h3:before, h4:before, h5:before, h6:before { -webkit-animation: avatar_turn_around 1s linear infinite; -moz-animation: avatar_turn_around 1s linear infinite; -o-animation: avatar_turn_around 1s linear infinite; -ms-animation: avatar_turn_around 1s linear infinite; animation: avatar_turn_around 1s linear infinite; }</style><link rel="stylesheet" href="/css/copyright.css"><link rel="stylesheet" href="/css/modify.styl"><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://picture-typora-bucket.oss-cn-shanghai.aliyuncs.com/typora/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">342</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/love/"><i class="fa-fw fas fa-gift"></i><span> Love</span></a></div></div></div></div><div id="body-wrap"><header class="post-bg" id="page-header" style="background: transparent"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">John Doe</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/love/"><i class="fa-fw fas fa-gift"></i><span> Love</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Scrapy介绍</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-16T12:14:00.000Z" title="发表于 2021-12-16 20:14:00">2021-12-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-12-16T13:33:16.857Z" title="更新于 2021-12-16 21:33:16">2021-12-16</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Scrapy介绍"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Scrapy介绍"><a href="#Scrapy介绍" class="headerlink" title="Scrapy介绍"></a>Scrapy介绍</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#1  通用的网络爬虫框架,爬虫界的django</span><br><span class="line">#2 scrapy执行流程</span><br><span class="line">    5大组件</span><br><span class="line">        -引擎(EGINE)：大总管，负责控制数据的流向</span><br><span class="line">        -调度器(SCHEDULER)：由它来决定下一个要抓取的网址是什么，去除重复的网址(集合，布隆过滤器)</span><br><span class="line">            -深度优先</span><br><span class="line">                -摁着一条线爬取</span><br><span class="line">                -先进先出</span><br><span class="line">            -广度优先</span><br><span class="line">                -后进先出</span><br><span class="line">                </span><br><span class="line">        -下载器(DOWLOADER)：用于下载网页内容, 并将网页内容返回给EGINE，下载器是建立在twisted这个高效的异步模型上的</span><br><span class="line">        -爬虫(SPIDERS):开发人员自定义的类，用来解析responses，并且提取items，或者发送新的请求request</span><br><span class="line">        -项目管道(ITEM PIPLINES):在items被提取后负责处理它们，主要包括清理、验证、持久化（比如存到数据库）等操作</span><br><span class="line">    2大中间件     </span><br><span class="line">        -爬虫中间件：位于EGINE和SPIDERS之间，主要工作是处理SPIDERS的输入和输出（用的很少）</span><br><span class="line">        -下载中间件：引擎和下载器之间，加代理，加头，集成selenium        </span><br><span class="line">        </span><br><span class="line"># 3 开发者只需要在固定的位置写固定的代码即可（写的最多的spider）</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Scrapy安装"><a href="#Scrapy安装" class="headerlink" title="Scrapy安装"></a>Scrapy安装</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#1 pip3 install scrapy（mac，linux）</span><br><span class="line">#2 windows上（80%能成功，少部分人成功不了）</span><br><span class="line">    1、pip3 install wheel #安装后，便支持通过wheel文件安装软件，wheel文件官网：https://www.lfd.uci.edu/~gohlke/pythonlibs</span><br><span class="line">    3、pip3 install lxml</span><br><span class="line">    4、pip3 install pyopenssl</span><br><span class="line">    5、下载并安装pywin32：https://sourceforge.net/projects/pywin32/files/pywin32/</span><br><span class="line">    6、下载twisted的wheel文件：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span><br><span class="line">    7、执行pip3 install 下载目录\Twisted-17.9.0-cp36-cp36m-win_amd64.whl</span><br><span class="line">    8、pip3 install scrapy</span><br><span class="line"># 3 就有scrapy命令</span><br><span class="line">    -D:\Python36\Scripts\scrapy.exe  用于创建项目</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Scrapy创建项目，创建爬虫，运行爬虫"><a href="#Scrapy创建项目，创建爬虫，运行爬虫" class="headerlink" title="Scrapy创建项目，创建爬虫，运行爬虫"></a>Scrapy创建项目，创建爬虫，运行爬虫</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">1 scrapy startproject 项目名</span><br><span class="line">    -scrapy startproject firstscrapy</span><br><span class="line">2 创建爬虫</span><br><span class="line">    -scrapy genspider example example.com</span><br><span class="line">    -scrapy genspider 爬虫名 爬虫地址</span><br><span class="line">    -scrapy genspider chouti dig.chouti.com</span><br><span class="line">    -一执行就会在spider文件夹下创建出一个py文件，名字叫chouti</span><br><span class="line">3 运行爬虫</span><br><span class="line">    -scrapy crawl chouti   # 带运行日志</span><br><span class="line">    -scrapy crawl chouti --nolog  # 不带日志</span><br><span class="line">4 支持右键执行爬虫</span><br><span class="line">    -在项目路径下新建一个main.py</span><br><span class="line">    from scrapy.cmdline import execute</span><br><span class="line">    execute(['scrapy','crawl','chouti','--nolog'])</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Scrapy目录介绍"><a href="#Scrapy目录介绍" class="headerlink" title="Scrapy目录介绍"></a>Scrapy目录介绍</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 目录介绍</span><br><span class="line">firstscrapy  # 项目名字</span><br><span class="line">    firstscrapy # 包</span><br><span class="line">        -spiders # 所有的爬虫文件放在里面</span><br><span class="line">            -baidu.py # 一个个的爬虫（以后基本上都在这写东西）</span><br><span class="line">            -chouti.py</span><br><span class="line">        -middlewares.py # 中间件（爬虫，下载中间件都写在这）</span><br><span class="line">        -pipelines.py   # 持久化相关写在这（items.py中类的对象）</span><br><span class="line">        -main.py        # 自己加的，执行爬虫</span><br><span class="line">        -items.py       # 一个一个的类，</span><br><span class="line">        -settings.py    # 配置文件</span><br><span class="line">    scrapy.cfg          # 上线相关</span><br></pre></td></tr></tbody></table></figure>

<h2 id="settings介绍"><a href="#settings介绍" class="headerlink" title="settings介绍"></a>settings介绍</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1 默认情况，scrapy会去遵循爬虫协议</span><br><span class="line">2 修改配置文件参数，强行爬取，不遵循协议</span><br><span class="line">    -ROBOTSTXT_OBEY = False</span><br><span class="line">3 USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36'</span><br><span class="line">4 LOG_LEVEL='ERROR'</span><br></pre></td></tr></tbody></table></figure>

<h2 id="爬取抽屉新闻"><a href="#爬取抽屉新闻" class="headerlink" title="爬取抽屉新闻"></a>爬取抽屉新闻</h2><p><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" alt="img"></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">from scrapy.http.request import Request</span><br><span class="line">from firstscrapy.items import ChoutiItem</span><br><span class="line"></span><br><span class="line">class ChoutiSpider(scrapy.Spider):</span><br><span class="line">    name = 'chouti'</span><br><span class="line">    allowed_domains = ['dig.chouti.com']</span><br><span class="line">    start_urls = ['http://dig.chouti.com/']</span><br><span class="line"></span><br><span class="line">    #</span><br><span class="line">    #     # 使用第三方解析</span><br><span class="line">    #     # def parse(self, response):</span><br><span class="line">    #     #     # print(response.text)</span><br><span class="line">    #     #     # 解析数据(第一种方案，自己解析，bs4,lxml)</span><br><span class="line">    #     #     soup = BeautifulSoup(response.text,'lxml')</span><br><span class="line">    #     #     divs = soup.find_all(class_='link-title')</span><br><span class="line">    #     #     for div in divs:</span><br><span class="line">    #     #         print(div.text)</span><br><span class="line">    #</span><br><span class="line">    #     # 想继续爬取其他网址</span><br><span class="line">    #     # def parse(self, response):</span><br><span class="line">    #     #     # 以后解析都在这里</span><br><span class="line">    #     #     print(response.status)</span><br><span class="line">    #     #     # 假设解析出一个网址(继续爬取)</span><br><span class="line">    #     #     return Request('https://www.baidi.com/',dont_filter=True)</span><br><span class="line">    #</span><br><span class="line">    #     # 使用自带解析</span><br><span class="line">    #     # xpath</span><br><span class="line">    #     # xpath:response.xpath('xpath语法').extract()  取所有</span><br><span class="line">    #     # xpath:response.xpath('xpath语法').extract_first()  取一个</span><br><span class="line">    #</span><br><span class="line">    #     # css</span><br><span class="line">    #     # response.css('.link-title::text').extract()  # 取文本</span><br><span class="line">    #     # response.css('.link-title::attr(href)').extract()[0]  # 取属性</span><br><span class="line">    #     def parse(self, response):</span><br><span class="line">    #         # 只有css和xpath</span><br><span class="line">    #         # title_list = response.css('.link-title')</span><br><span class="line">    #         # print(len(title_list))</span><br><span class="line">    #         # title_list = response.xpath('//a[contains(@class,"link-title")]/text()').extract()</span><br><span class="line">    #         # title_list = response.xpath('//a[contains(@class,"link-title")]').extract()</span><br><span class="line">    #         # print(len(title_list))</span><br><span class="line">    #         # print(title_list)</span><br><span class="line">    #</span><br><span class="line">    #         # 解析出所有的标题和图片地址</span><br><span class="line">    #         div_list = response.xpath('//div[contains(@class,"link-item")]')</span><br><span class="line">    #         for div in div_list:</span><br><span class="line">    #             # extract() 取出列表 即便有一个也是列表</span><br><span class="line">    #             # extract_first() 取出第一个值</span><br><span class="line">    #             # title = div.css('.link-title::text').extract()</span><br><span class="line">    #             # title = div.css('.link-title::text').extract_first()</span><br><span class="line">    #             url = div.css('.link-title::attr(href)').extract()[0]</span><br><span class="line">    #             # print(title)</span><br><span class="line">    #             print(url)</span><br><span class="line"></span><br><span class="line">    # 持久化方案一</span><br><span class="line">    # def parse(self, response, **kwargs):</span><br><span class="line">    #     ll = []</span><br><span class="line">    #     div_list = response.xpath('//div[contains(@class,"link-item")]')</span><br><span class="line">    #     for div in div_list:</span><br><span class="line">    #         title = div.css('.link-title::text').extract()</span><br><span class="line">    #         url = div.css('.link-title::attr(href)').extract_first()</span><br><span class="line">    #         phone_url = div.css('.image-scale::attr(src)').extract_first()</span><br><span class="line">    #         # 持久化：方案一(用得少)parser必须返回列表套字典的形式</span><br><span class="line">    #         ll.append({'title': title, 'url': url, 'phone_url': phone_url})</span><br><span class="line">    #     return ll</span><br><span class="line"></span><br><span class="line">    # 持久化方案二</span><br><span class="line">    def parse(self, response):</span><br><span class="line"></span><br><span class="line">        div_list = response.xpath('//div[contains(@class,"link-item")]')</span><br><span class="line">        for div in div_list:</span><br><span class="line">            item = ChoutiItem()</span><br><span class="line">            title = div.css('.link-title::text').extract_first()</span><br><span class="line">            url = div.css('.link-title::attr(href)').extract_first()</span><br><span class="line">            photo_url = div.css('.image-scale::attr(src)').extract_first()</span><br><span class="line">            if not photo_url:</span><br><span class="line">                photo_url = ''</span><br><span class="line">            # item.title=title</span><br><span class="line">            # item.url=url</span><br><span class="line">            # item.photo_url=photo_url</span><br><span class="line">            item['title'] = title</span><br><span class="line">            item['url'] = url</span><br><span class="line">            item['photo_url'] = photo_url</span><br><span class="line">            yield item</span><br></pre></td></tr></tbody></table></figure>

<p>View Code</p>
<h2 id="Scrapy的数据解析"><a href="#Scrapy的数据解析" class="headerlink" title="Scrapy的数据解析"></a>Scrapy的数据解析</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#xpath：</span><br><span class="line">    -response.xpath('//a[contains(@class,"link-title")]/text()').extract()  # 取文本</span><br><span class="line">    -response.xpath('//a[contains(@class,"link-title")]/@href').extract()  #取属性</span><br><span class="line">#css</span><br><span class="line">    -response.css('.link-title::text').extract()  # 取文本</span><br><span class="line">    -response.css('.link-title::attr(href)').extract_first()  # 取属性</span><br></pre></td></tr></tbody></table></figure>

<h2 id="Scrapy的持久化存储"><a href="#Scrapy的持久化存储" class="headerlink" title="Scrapy的持久化存储"></a>Scrapy的持久化存储</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#1 方案一：parser函数必须返回列表套字典的形式（了解）</span><br><span class="line">    scrapy crawl chouti -o chouti.csv</span><br><span class="line">#2 方案二：高级，pipline item存储（mysql，redis，文件）</span><br><span class="line">    -在Items.py中写一个类</span><br><span class="line">    -在spinder中导入，实例化，把数据放进去</span><br><span class="line">            item['title']=title</span><br><span class="line">            item['url']=url</span><br><span class="line">            item['photo_url']=photo_url</span><br><span class="line">            yield item</span><br><span class="line">            </span><br><span class="line">    -在setting中配置（数字越小，级别越高）</span><br><span class="line">        ITEM_PIPELINES = {</span><br><span class="line">           'firstscrapy.pipelines.ChoutiFilePipeline': 300,</span><br><span class="line">        }</span><br><span class="line">    -在pipelines.py中写ChoutiFilePipeline</span><br><span class="line">        -open_spider（开始的时候）</span><br><span class="line">        -close_spider（结束的时候）</span><br><span class="line">        -process_item（在这持久化）</span><br></pre></td></tr></tbody></table></figure>

<h2 id="自动给抽屉点赞"><a href="#自动给抽屉点赞" class="headerlink" title="自动给抽屉点赞"></a>自动给抽屉点赞</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">from selenium import webdriver</span><br><span class="line">import time</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">bro=webdriver.Chrome(executable_path='./chromedriver.exe')</span><br><span class="line">bro.implicitly_wait(10)</span><br><span class="line">bro.get('https://dig.chouti.com/')</span><br><span class="line"></span><br><span class="line">login_b=bro.find_element_by_id('login_btn')</span><br><span class="line">print(login_b)</span><br><span class="line">login_b.click()</span><br><span class="line"></span><br><span class="line">username=bro.find_element_by_name('phone')</span><br><span class="line">username.send_keys('18953675221')</span><br><span class="line">password=bro.find_element_by_name('password')</span><br><span class="line">password.send_keys('lqz123')</span><br><span class="line"></span><br><span class="line">button=bro.find_element_by_css_selector('button.login-btn')</span><br><span class="line">button.click()</span><br><span class="line"># 可能有验证码，手动操作一下</span><br><span class="line">time.sleep(10)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">my_cookie=bro.get_cookies()  # 列表</span><br><span class="line">print(my_cookie)</span><br><span class="line">bro.close()</span><br><span class="line"></span><br><span class="line"># 这个cookie不是一个字典，不能直接给requests使用，需要转一下</span><br><span class="line">cookie={}</span><br><span class="line">for item in my_cookie:</span><br><span class="line">    cookie[item['name']]=item['value']</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">headers = {</span><br><span class="line">    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.105 Safari/537.36',</span><br><span class="line">    'Referer': 'https://dig.chouti.com/'}</span><br><span class="line"># ret = requests.get('https://dig.chouti.com/',headers=headers)</span><br><span class="line"># print(ret.text)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ret=requests.get('https://dig.chouti.com/top/24hr?_=1596677637670',headers=headers)</span><br><span class="line">print(ret.json())</span><br><span class="line">ll=[]</span><br><span class="line">for item in ret.json()['data']:</span><br><span class="line">    ll.append(item['id'])</span><br><span class="line"></span><br><span class="line">print(ll)</span><br><span class="line">for id in ll:</span><br><span class="line">    ret=requests.post(' https://dig.chouti.com/link/vote',headers=headers,cookies=cookie,data={'linkId':id})</span><br><span class="line">    print(ret.text)</span><br><span class="line"></span><br><span class="line">'https://dig.chouti.com/comments/create'</span><br><span class="line">'''</span><br><span class="line">content: 说的号</span><br><span class="line">linkId: 29829529</span><br><span class="line">parentId: 0</span><br><span class="line"></span><br><span class="line">'''</span><br></pre></td></tr></tbody></table></figure>

<h2 id="全站爬取cnblogs"><a href="#全站爬取cnblogs" class="headerlink" title="全站爬取cnblogs"></a>全站爬取cnblogs</h2><p><img src="https://images.cnblogs.com/OutliningIndicators/ContractedBlock.gif" alt="img"></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">import scrapy</span><br><span class="line">from cnblogs.items import CnblogsItem</span><br><span class="line">from scrapy import Request</span><br><span class="line">from selenium import webdriver</span><br><span class="line">from scrapy.dupefilters import RFPDupeFilter</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class CnblogSpider(scrapy.Spider):</span><br><span class="line">    name = 'cnblog'</span><br><span class="line">    allowed_domains = ['www.cnblogs.com']</span><br><span class="line">    start_urls = ['https://www.cnblogs.com/']</span><br><span class="line">    bro = webdriver.Chrome(executable_path='../../chromedriver.exe')</span><br><span class="line"></span><br><span class="line">    '''</span><br><span class="line">    爬取原则：scrapy默认是先进先出</span><br><span class="line">        -深度优先：详情页先爬  队列：先进去先出来</span><br><span class="line">        -广度优先：每一页先爬  栈：先进后出</span><br><span class="line">    '''</span><br><span class="line">    def parse(self, response):</span><br><span class="line">        # print(response.text)</span><br><span class="line">        div_list = response.css('article.post-item')</span><br><span class="line">        for div in div_list:</span><br><span class="line">            item = CnblogsItem()</span><br><span class="line">            title = div.xpath('.//div[1]/a/text()').extract_first()</span><br><span class="line">            item['title'] = title</span><br><span class="line">            url = div.xpath('.//div[1]/a/@href').extract_first()</span><br><span class="line">            item['url'] = url</span><br><span class="line">            desc = div.xpath('string(.//div[1]/p)').extract_first().strip()</span><br><span class="line">            item['desc'] = desc</span><br><span class="line">            # print(title)</span><br><span class="line">            # print(url)</span><br><span class="line">            # print(desc)</span><br><span class="line"></span><br><span class="line">            # 继续爬取详情</span><br><span class="line">            # callback如果不写默认回调到parse方法</span><br><span class="line">            # 如果写了，响应回来的对象就会调到自己写的解析方法中</span><br><span class="line">            # 请求和响应之间传递参数，使用meta</span><br><span class="line"></span><br><span class="line">            yield Request(url, callback=self.parser_detail, meta={'item': item})</span><br><span class="line"></span><br><span class="line">        # 解析出下页的地址</span><br><span class="line">        next = 'https://www.cnblogs.com' + response.css('#paging_block&gt;div a:last-child::attr(href)').extract_first()</span><br><span class="line">        # print(next)</span><br><span class="line">        yield Request(next)</span><br><span class="line"></span><br><span class="line">    def parser_detail(self, response):</span><br><span class="line">        content = response.css('#cnblogs_post_body').extract_first()</span><br><span class="line">        print(str(content))</span><br><span class="line">        # item哪里来</span><br><span class="line">        item = response.meta.get('item')</span><br><span class="line">        item['content'] = content</span><br><span class="line">        yield item</span><br></pre></td></tr></tbody></table></figure>

<p>View Code</p>
<h2 id="Scrapy的请求传参"><a href="#Scrapy的请求传参" class="headerlink" title="Scrapy的请求传参"></a>Scrapy的请求传参</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 把要传递的数据放到meta中</span><br><span class="line">yield Request(urlmeta={'item':item})</span><br><span class="line"># 在response对象中取出来</span><br><span class="line">item=response.meta.get('item')</span><br></pre></td></tr></tbody></table></figure>

<h2 id="提升scrapy爬取数据的效率"><a href="#提升scrapy爬取数据的效率" class="headerlink" title="提升scrapy爬取数据的效率"></a>提升scrapy爬取数据的效率</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">- 在配置文件中进行相关的配置即可:(默认还有一套setting)</span><br><span class="line">#1 增加并发：</span><br><span class="line">IO密集型</span><br><span class="line">默认scrapy开启的并发线程为32个，可以适当进行增加。在settings配置文件中修改CONCURRENT_REQUESTS = 100值为100,并发设置成了为100。</span><br><span class="line"></span><br><span class="line">#2 降低日志级别：</span><br><span class="line">在运行scrapy时，会有大量日志信息的输出，为了减少CPU的使用率。可以设置log输出信息为INFO或者ERROR即可。在配置文件中编写：LOG_LEVEL = ‘INFO’</span><br><span class="line"></span><br><span class="line"># 3 禁止cookie：</span><br><span class="line">如果不是真的需要cookie，则在scrapy爬取数据时可以禁止cookie从而减少CPU的使用率，提升爬取效率。在配置文件中编写：COOKIES_ENABLED = False</span><br><span class="line"></span><br><span class="line"># 4禁止重试：</span><br><span class="line">对失败的HTTP进行重新请求（重试）会减慢爬取速度，因此可以禁止重试。在配置文件中编写：RETRY_ENABLED = False</span><br><span class="line"></span><br><span class="line"># 5 减少下载超时：</span><br><span class="line">如果对一个非常慢的链接进行爬取，减少下载超时可以能让卡住的链接快速被放弃，从而提升效率。在配置文件中进行编写：DOWNLOAD_TIMEOUT = 10 超时时间为10s</span><br></pre></td></tr></tbody></table></figure>

<h2 id="scrapy的中间件-下载中间件"><a href="#scrapy的中间件-下载中间件" class="headerlink" title="scrapy的中间件(下载中间件)"></a>scrapy的中间件(下载中间件)</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># 1 都写在middlewares.py</span><br><span class="line"># 2 爬虫中间件</span><br><span class="line"># 3 下载中间件</span><br><span class="line"># 4 要生效，一定要配置，配置文件</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 下载中间件</span><br><span class="line">-process_request：返回不同的对象，后续处理不同（加代理...）</span><br><span class="line">          # 1 更换请求头</span><br><span class="line">        # print(type(request.headers))</span><br><span class="line">        # print(request.headers)</span><br><span class="line">        #</span><br><span class="line">        # from scrapy.http.headers import Headers</span><br><span class="line">        # request.headers['User-Agent']=''</span><br><span class="line"></span><br><span class="line">        # 2 加cookie ---cookie池</span><br><span class="line">        # 假设你你已经搭建好cookie 池了，</span><br><span class="line">        # print('00000--',request.cookies)</span><br><span class="line">        # request.cookies={'username':'asdfasdf'}</span><br><span class="line"></span><br><span class="line">        # 3 加代理</span><br><span class="line">        # print(request.meta)</span><br><span class="line">        # request.meta['download_timeout'] = 20</span><br><span class="line">        # request.meta["proxy"] = 'http://27.188.62.3:8060'</span><br><span class="line">-process_response：返回不同的对象，后续处理不同</span><br><span class="line">- process_exception</span><br><span class="line">def process_exception(self, request, exception, spider):</span><br><span class="line">        print('xxxx')</span><br><span class="line">        # 不允许直接改url</span><br><span class="line">        # request.url='https://www.baidu.com'</span><br><span class="line">        from scrapy import Request</span><br><span class="line">        request=Request(url='https://www.baidu.com',callback=spider.parser)</span><br><span class="line">        return request</span><br></pre></td></tr></tbody></table></figure>

<h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://img2020.cnblogs.com/blog/2061671/202106/2061671-20210615154214759-1504551155.png" alt="img"></h2><h2 id="selenium在scrapy中的使用流程"><a href="#selenium在scrapy中的使用流程" class="headerlink" title="selenium在scrapy中的使用流程"></a>selenium在scrapy中的使用流程</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># 当前爬虫用的selenium是同一个</span><br><span class="line"></span><br><span class="line"># 1 在爬虫中初始化webdriver对象</span><br><span class="line">    from selenium import webdriver</span><br><span class="line">    class CnblogSpider(scrapy.Spider):</span><br><span class="line">        name = 'cnblog'</span><br><span class="line">        ...</span><br><span class="line"> bro=webdriver.Chrome(executable_path='../chromedriver.exe')</span><br><span class="line"># 2 在中间件中使用（process_request）</span><br><span class="line">spider.bro.get('https://dig.chouti.com/')   response=HtmlResponse(url='https://dig.chouti.com/',body=spider.bro.page_source.encode('utf-8'),request=request)</span><br><span class="line">    return response</span><br><span class="line">    </span><br><span class="line"># 3 在爬虫中关闭</span><br><span class="line">    def close(self, reason):</span><br><span class="line">        print("我结束了")</span><br><span class="line">        self.bro.close()</span><br></pre></td></tr></tbody></table></figure>

<h2 id="去重规则源码分析"><a href="#去重规则源码分析" class="headerlink" title="去重规则源码分析"></a>去重规则源码分析</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from scrapy.dupefilters import RFPDupeFilter</span><br><span class="line"># 详见代码</span><br></pre></td></tr></tbody></table></figure>

<h2 id="布隆过滤器"><a href="#布隆过滤器" class="headerlink" title="布隆过滤器"></a>布隆过滤器</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">详情参见：https://www.cnblogs.com/xiaoyuanqujing/protected/articles/11969224.html</span><br><span class="line"></span><br><span class="line">极小内存校验是否重复</span><br><span class="line"></span><br><span class="line">#python3.6 安装</span><br><span class="line">#需要先安装bitarray</span><br><span class="line">pip3 install bitarray-0.8.1-cp36-cp36m-win_amd64.whl（pybloom_live依赖这个包，需要先安装）</span><br><span class="line">#下载地址：https://www.lfd.uci.edu/~gohlke/pythonlibs/</span><br><span class="line">pip3 install pybloom_live</span><br></pre></td></tr></tbody></table></figure>

<h2 id="分布式爬虫-scrapy-redis"><a href="#分布式爬虫-scrapy-redis" class="headerlink" title="分布式爬虫(scrapy-redis)"></a>分布式爬虫(scrapy-redis)</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 1 pip3 install scrapy-redis</span><br><span class="line"># 2 原来继承Spider，现在继承RedisSpider</span><br><span class="line"># 3 不能写start_urls = ['https:/www.cnblogs.com/']</span><br><span class="line"># 4 需要写redis_key = 'myspider:start_urls'</span><br><span class="line"># 5 setting中配置：</span><br><span class="line"></span><br><span class="line"># redis的连接</span><br><span class="line">REDIS_HOST = 'localhost'                            # 主机名</span><br><span class="line">REDIS_PORT = 6379                                   # 端口</span><br><span class="line">    # 使用scrapy-redis的去重</span><br><span class="line">DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"</span><br><span class="line"># 使用scrapy-redis的Scheduler</span><br><span class="line"># 分布式爬虫的配置</span><br><span class="line">SCHEDULER = "scrapy_redis.scheduler.Scheduler"</span><br><span class="line"># 持久化的可以配置，也可以不配置</span><br><span class="line">ITEM_PIPELINES = {</span><br><span class="line">   'scrapy_redis.pipelines.RedisPipeline': 299</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 9现在要让爬虫运行起来，需要去redis中以myspider:start_urls为key，插入一个起始地址lpush myspider:start_urls https://www.cnblogs.com/</span><br></pre></td></tr></tbody></table></figure>

<h2 id="破解知乎登录-js逆向和解密"><a href="#破解知乎登录-js逆向和解密" class="headerlink" title="破解知乎登录(js逆向和解密)"></a>破解知乎登录(js逆向和解密)</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line">client_id=c3cef7c66a1843f8b3a9e6a1e3160e20&amp;</span><br><span class="line">grant_type=password&amp;</span><br><span class="line">timestamp=1596702006088&amp;</span><br><span class="line">source=com.zhihu.web&amp;</span><br><span class="line">signature=eac4a6c461f9edf86ef33ef950c7b6aa426dbb39&amp;</span><br><span class="line">username=%2B86liuqingzheng&amp;</span><br><span class="line">password=1111111&amp;</span><br><span class="line">captcha=&amp;</span><br><span class="line">lang=en&amp;</span><br><span class="line">utm_source=&amp;</span><br><span class="line">ref_source=other_https%3A%2F%2Fwww.zhihu.com%2Fsignin%3Fnext%3D%252F"</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 破解知乎登陆</span><br><span class="line"></span><br><span class="line">import requests    #请求解析库</span><br><span class="line"></span><br><span class="line">import base64                              #base64解密加密库</span><br><span class="line">from PIL import Image                        #图片处理库</span><br><span class="line">import hmac                                  #加密库</span><br><span class="line">from hashlib import sha1                  #加密库</span><br><span class="line">import time</span><br><span class="line">from urllib.parse import urlencode          #url编码库</span><br><span class="line">import execjs                              #python调用node.js</span><br><span class="line">from http import cookiejar as cookielib</span><br><span class="line">class Spider():</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.session = requests.session()</span><br><span class="line">        self.session.cookies = cookielib.LWPCookieJar()    #使cookie可以调用save和load方法</span><br><span class="line">        self.login_page_url = 'https://www.zhihu.com/signin?next=%2F'</span><br><span class="line">        self.login_api = 'https://www.zhihu.com/api/v3/oauth/sign_in'</span><br><span class="line">        self.captcha_api = 'https://www.zhihu.com/api/v3/oauth/captcha?lang=en'</span><br><span class="line">        self.headers = {</span><br><span class="line">            'user-agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER',</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        self.captcha =''         #存验证码</span><br><span class="line">        self.signature = ''       #存签名</span><br><span class="line"></span><br><span class="line">    # 首次请求获取cookie</span><br><span class="line">    def get_base_cookie(self):</span><br><span class="line">        self.session.get(url=self.login_page_url, headers=self.headers)</span><br><span class="line"></span><br><span class="line">    def deal_captcha(self):</span><br><span class="line">        r = self.session.get(url=self.captcha_api, headers=self.headers)</span><br><span class="line">        r = r.json()</span><br><span class="line">        if r.get('show_captcha'):</span><br><span class="line">            while True:</span><br><span class="line">                r = self.session.put(url=self.captcha_api, headers=self.headers)</span><br><span class="line">                img_base64 = r.json().get('img_base64')</span><br><span class="line">                with open('captcha.png', 'wb') as f:</span><br><span class="line">                    f.write(base64.b64decode(img_base64))</span><br><span class="line">                captcha_img = Image.open('captcha.png')</span><br><span class="line">                captcha_img.show()</span><br><span class="line">                self.captcha = input('输入验证码:')</span><br><span class="line">                r = self.session.post(url=self.captcha_api, data={'input_text': self.captcha},</span><br><span class="line">                                      headers=self.headers)</span><br><span class="line">                if r.json().get('success'):</span><br><span class="line">                    break</span><br><span class="line"></span><br><span class="line">    def get_signature(self):</span><br><span class="line">        # 生成加密签名</span><br><span class="line">        a = hmac.new(b'd1b964811afb40118a12068ff74a12f4', digestmod=sha1)</span><br><span class="line">        a.update(b'password')</span><br><span class="line">        a.update(b'c3cef7c66a1843f8b3a9e6a1e3160e20')</span><br><span class="line">        a.update(b'com.zhihu.web')</span><br><span class="line">        a.update(str(int(time.time() * 1000)).encode('utf-8'))</span><br><span class="line">        self.signature = a.hexdigest()</span><br><span class="line"></span><br><span class="line">    def post_login_data(self):</span><br><span class="line">        data = {</span><br><span class="line">            'client_id': 'c3cef7c66a1843f8b3a9e6a1e3160e20',</span><br><span class="line">            'grant_type': 'password',</span><br><span class="line">            'timestamp': str(int(time.time() * 1000)),</span><br><span class="line">            'source': 'com.zhihu.web',</span><br><span class="line">            'signature': self.signature,</span><br><span class="line">            'username': '+8618953675221',</span><br><span class="line">            'password': '',</span><br><span class="line">            'captcha': self.captcha,</span><br><span class="line">            'lang': 'en',</span><br><span class="line">            'utm_source': '',</span><br><span class="line">            'ref_source': 'other_https://www.zhihu.com/signin?next=%2F',</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        headers = {</span><br><span class="line">            'x-zse-83': '3_2.0',</span><br><span class="line">            'content-type': 'application/x-www-form-urlencoded',</span><br><span class="line">            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/57.0.2987.98 Safari/537.36 LBBROWSER',</span><br><span class="line">        }</span><br><span class="line"></span><br><span class="line">        data = urlencode(data)</span><br><span class="line">        with open('zhih.js', 'rt', encoding='utf-8') as f:</span><br><span class="line">            js = execjs.compile(f.read(), cwd='node_modules')</span><br><span class="line">        data = js.call('b', data)</span><br><span class="line"></span><br><span class="line">        r = self.session.post(url=self.login_api, headers=headers, data=data)</span><br><span class="line">        print(r.text)</span><br><span class="line">        if r.status_code == 201:</span><br><span class="line">            self.session.cookies.save('mycookie')</span><br><span class="line">            print('登录成功')</span><br><span class="line">        else:</span><br><span class="line">            print('登录失败')</span><br><span class="line"></span><br><span class="line">    def login(self):</span><br><span class="line">        self.get_base_cookie()</span><br><span class="line">        self.deal_captcha()</span><br><span class="line">        self.get_signature()</span><br><span class="line">        self.post_login_data()</span><br><span class="line">if __name__ == '__main__':</span><br><span class="line">    zhihu_spider = Spider()</span><br><span class="line">    zhihu_spider.login()</span><br></pre></td></tr></tbody></table></figure>

<h2 id="爬虫的反扒措施"><a href="#爬虫的反扒措施" class="headerlink" title="爬虫的反扒措施"></a>爬虫的反扒措施</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1 user-agent</span><br><span class="line">2 referer</span><br><span class="line">3 cookie（cookie池，先访问一次）</span><br><span class="line">4 频率限制（代理池，延迟）</span><br><span class="line">5 js加密（扣出来，exjs模块执行）</span><br><span class="line">6 css加密</span><br><span class="line">7 验证码（打码平台），半手动</span><br><span class="line">8 图片懒加载</span><br></pre></td></tr></tbody></table></figure>

</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Scrapy介绍</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="http://www.zhangxiaotao.live/2021/12/16/%E7%88%AC%E8%99%AB/Scrapy%E6%A1%86%E6%9E%B6/">http://www.zhangxiaotao.live/2021/12/16/%E7%88%AC%E8%99%AB/Scrapy%E6%A1%86%E6%9E%B6/</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a" style="display: inline-block;width: 120px"><h>作者</h><div class="post-copyright-cc-info"><h>John Doe</h></div></div><div class="post-copyright-c" style="display: inline-block;width: 120px"><h>发布于</h><div class="post-copyright-cc-info"><h>2021-12-16</h></div></div><div class="post-copyright-u" style="display: inline-block;width: 120px"><h>更新于</h><div class="post-copyright-cc-info"><h>2021-12-16</h></div></div><div class="post-copyright-c" style="display: inline-block;width: 120px"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY 4.0" href="https://creativecommons.org/licenses/by/4.0/deed.zh">CC BY 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer=""></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/12/16/%E7%88%AC%E8%99%AB/04-02%20%E5%AD%98%E5%82%A8%E5%BA%93%E4%B9%8Bredis%20/"><img class="prev-cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">存储库之redis</div></div></a></div><div class="next-post pull-right"><a href="/2021/12/16/%E7%88%AC%E8%99%AB/04-01%20%E5%AD%98%E5%82%A8%E5%BA%93%E4%B9%8BMongoDB%20/"><img class="next-cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">存储库之MongoDB</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2021/12/16/%E7%88%AC%E8%99%AB/01-01%E7%88%AC%E8%99%AB%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86/" title="爬虫基本原理"><img class="cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-16</div><div class="title">爬虫基本原理</div></div></a></div><div><a href="/2021/12/16/%E7%88%AC%E8%99%AB/03-01%20%E8%A7%A3%E6%9E%90%E5%BA%93beautifulsoup%20/" title="解析库beautifulsoup"><img class="cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-16</div><div class="title">解析库beautifulsoup</div></div></a></div><div><a href="/2021/12/16/%E7%88%AC%E8%99%AB/05-01%20%E7%88%AC%E8%99%AB%E9%AB%98%E6%80%A7%E8%83%BD%E7%9B%B8%E5%85%B3%20/" title="爬虫高性能相关"><img class="cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-16</div><div class="title">爬虫高性能相关</div></div></a></div><div><a href="/2021/12/16/%E7%88%AC%E8%99%AB/%E6%90%AD%E5%BB%BA%E5%85%8D%E8%B4%B9%E4%BB%A3%E7%90%86%E6%B1%A0%20/" title="搭建免费代理池"><img class="cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-16</div><div class="title">搭建免费代理池</div></div></a></div><div><a href="/2021/12/16/%E7%88%AC%E8%99%AB/02-01%20%E8%AF%B7%E6%B1%82%E5%BA%93%E4%B9%8Brequests%E5%BA%93%20/" title="请求库之requests库"><img class="cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-16</div><div class="title">请求库之requests库</div></div></a></div><div><a href="/2021/12/16/%E7%88%AC%E8%99%AB/02-02%20%E7%88%AC%E8%99%AB%E8%AF%B7%E6%B1%82%E5%BA%93%E4%B9%8Bselenium/" title="爬虫请求库之selenium"><img class="cover" src="https://img0.baidu.com/it/u=1896679204,784253148&amp;fm=26&amp;fmt=auto" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-12-16</div><div class="title">爬虫请求库之selenium</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://picture-typora-bucket.oss-cn-shanghai.aliyuncs.com/typora/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"></div><div class="author-info__name">John Doe</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">342</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">33</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">36</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/zhzhang12138"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">This is my blog   <img src="https://cdn.jsdelivr.net/gh/CNhuazhu/Image/pig.gif"></div><timing></timing></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">Scrapy介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E5%AE%89%E8%A3%85"><span class="toc-number">2.</span> <span class="toc-text">Scrapy安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE%EF%BC%8C%E5%88%9B%E5%BB%BA%E7%88%AC%E8%99%AB%EF%BC%8C%E8%BF%90%E8%A1%8C%E7%88%AC%E8%99%AB"><span class="toc-number">3.</span> <span class="toc-text">Scrapy创建项目，创建爬虫，运行爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E7%9B%AE%E5%BD%95%E4%BB%8B%E7%BB%8D"><span class="toc-number">4.</span> <span class="toc-text">Scrapy目录介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#settings%E4%BB%8B%E7%BB%8D"><span class="toc-number">5.</span> <span class="toc-text">settings介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E5%8F%96%E6%8A%BD%E5%B1%89%E6%96%B0%E9%97%BB"><span class="toc-number">6.</span> <span class="toc-text">爬取抽屉新闻</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E7%9A%84%E6%95%B0%E6%8D%AE%E8%A7%A3%E6%9E%90"><span class="toc-number">7.</span> <span class="toc-text">Scrapy的数据解析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E7%9A%84%E6%8C%81%E4%B9%85%E5%8C%96%E5%AD%98%E5%82%A8"><span class="toc-number">8.</span> <span class="toc-text">Scrapy的持久化存储</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E7%BB%99%E6%8A%BD%E5%B1%89%E7%82%B9%E8%B5%9E"><span class="toc-number">9.</span> <span class="toc-text">自动给抽屉点赞</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%A8%E7%AB%99%E7%88%AC%E5%8F%96cnblogs"><span class="toc-number">10.</span> <span class="toc-text">全站爬取cnblogs</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E7%9A%84%E8%AF%B7%E6%B1%82%E4%BC%A0%E5%8F%82"><span class="toc-number">11.</span> <span class="toc-text">Scrapy的请求传参</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8F%90%E5%8D%87scrapy%E7%88%AC%E5%8F%96%E6%95%B0%E6%8D%AE%E7%9A%84%E6%95%88%E7%8E%87"><span class="toc-number">12.</span> <span class="toc-text">提升scrapy爬取数据的效率</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy%E7%9A%84%E4%B8%AD%E9%97%B4%E4%BB%B6-%E4%B8%8B%E8%BD%BD%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-number">13.</span> <span class="toc-text">scrapy的中间件(下载中间件)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link"><span class="toc-number">14.</span> <span class="toc-text"></span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#selenium%E5%9C%A8scrapy%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E6%B5%81%E7%A8%8B"><span class="toc-number">15.</span> <span class="toc-text">selenium在scrapy中的使用流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8E%BB%E9%87%8D%E8%A7%84%E5%88%99%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90"><span class="toc-number">16.</span> <span class="toc-text">去重规则源码分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8"><span class="toc-number">17.</span> <span class="toc-text">布隆过滤器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB-scrapy-redis"><span class="toc-number">18.</span> <span class="toc-text">分布式爬虫(scrapy-redis)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A0%B4%E8%A7%A3%E7%9F%A5%E4%B9%8E%E7%99%BB%E5%BD%95-js%E9%80%86%E5%90%91%E5%92%8C%E8%A7%A3%E5%AF%86"><span class="toc-number">19.</span> <span class="toc-text">破解知乎登录(js逆向和解密)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%9A%84%E5%8F%8D%E6%89%92%E6%8E%AA%E6%96%BD"><span class="toc-number">20.</span> <span class="toc-text">爬虫的反扒措施</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/02/22/Go/%E5%BE%AE%E6%9C%8D%E5%8A%A1/00%20%E8%A6%81%E8%BE%BE%E5%88%B0%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E8%A7%84%E6%A8%A1%E6%89%8D%E9%80%82%E5%90%88%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/" title="要达到什么样的规模才适合微服务架构"><img src="https://img0.baidu.com/it/u=3862707489,4221992336&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="要达到什么样的规模才适合微服务架构"></a><div class="content"><a class="title" href="/2022/02/22/Go/%E5%BE%AE%E6%9C%8D%E5%8A%A1/00%20%E8%A6%81%E8%BE%BE%E5%88%B0%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E8%A7%84%E6%A8%A1%E6%89%8D%E9%80%82%E5%90%88%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%B6%E6%9E%84/" title="要达到什么样的规模才适合微服务架构">要达到什么样的规模才适合微服务架构</a><time datetime="2022-02-22T07:29:00.000Z" title="发表于 2022-02-22 15:29:00">2022-02-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/25/Go/Go%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6/15%20golang%20cron%20%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/" title="golang cron 定时任务"><img src="https://img2.baidu.com/it/u=927172306,3765293649&amp;fm=26&amp;fmt=auto" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="golang cron 定时任务"></a><div class="content"><a class="title" href="/2022/01/25/Go/Go%E8%AF%AD%E8%A8%80%E8%BF%9B%E9%98%B6/15%20golang%20cron%20%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1/" title="golang cron 定时任务">golang cron 定时任务</a><time datetime="2022-01-25T00:37:00.000Z" title="发表于 2022-01-25 08:37:00">2022-01-25</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/22/Interview%20questions/01%20Go%20%E5%9F%BA%E7%A1%80%E7%B1%BB/" title="Go 基础类"><img src="https://img1.baidu.com/it/u=1233920880,3007053888&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=703&amp;h=500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go 基础类"></a><div class="content"><a class="title" href="/2022/01/22/Interview%20questions/01%20Go%20%E5%9F%BA%E7%A1%80%E7%B1%BB/" title="Go 基础类">Go 基础类</a><time datetime="2022-01-22T07:54:00.000Z" title="发表于 2022-01-22 15:54:00">2022-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/22/Interview%20questions/02%20Go%20%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" title="Go 并发编程"><img src="https://img1.baidu.com/it/u=1233920880,3007053888&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=703&amp;h=500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go 并发编程"></a><div class="content"><a class="title" href="/2022/01/22/Interview%20questions/02%20Go%20%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/" title="Go 并发编程">Go 并发编程</a><time datetime="2022-01-22T07:53:00.000Z" title="发表于 2022-01-22 15:53:00">2022-01-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/01/22/Interview%20questions/03%20Go%20Runtime/" title="Go Runtime"><img src="https://img1.baidu.com/it/u=1233920880,3007053888&amp;fm=253&amp;fmt=auto&amp;app=138&amp;f=JPEG?w=703&amp;h=500" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Go Runtime"></a><div class="content"><a class="title" href="/2022/01/22/Interview%20questions/03%20Go%20Runtime/" title="Go Runtime">Go Runtime</a><time datetime="2022-01-22T07:52:00.000Z" title="发表于 2022-01-22 15:52:00">2022-01-22</time></div></div></div></div></div></div></main><footer id="footer" style="background: transparent"><div id="footer-wrap"><div class="copyright">©2020 - 2022 <i id="heartbeat" class="fa fas fa-heartbeat"></i>  John Doe</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">Hi, welcome to my <a href="">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="algolia-search"><div class="search-dialog"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="/js/search/algolia.js"></script><div class="js-pjax"><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',()=> {preloader.endLoading()})
setTimeout(function(){preloader.endLoading();}, 3000);</script></div><div class="js-pjax"><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: '',
      appKey: '',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: 'nick,mail,link'.split(','),
      pageSize: '10',
      lang: 'zh-CN',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      path: window.location.pathname,
      requiredFields: ["nick,mail"],
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="/js/timing.js"></script><script src="https://cdn.jsdelivr.net/gh/weilain/cdn-photo/js/jquery.min.js"></script><script async="" data-pjax="" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script type="text/javascript" src="/js/crash_cheat.js"></script></body></html>